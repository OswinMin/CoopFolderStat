\documentclass[12pt, a4paper, oneside]{article}
\usepackage{amsmath, amsthm, amssymb, appendix, bm, graphicx, hyperref, mathrsfs, makecell}
\usepackage{bbm, stfloats, subfigure, pythonhighlight, CJK, algorithm, algorithmicx, algpseudocode}
\usepackage{geometry}
\geometry{a4paper,left=2.5cm,right=2.5cm,top=3cm,bottom=3cm}
\usepackage{fancyhdr}
\pagestyle{fancy}
\renewcommand\headrulewidth{.5pt}
\renewcommand\footrulewidth{0pt}
\setlength{\headheight}{15pt}
\fancyhead[L]{\textit{\leftmark}}
\fancyhead[R]{\thepage}

\title{\textbf{Localized Conformal Prediction}}
\author{Min}
\date{\today}
\linespread{1.6}
\definecolor{lightBlue}{rgb}{0.274,0.41,0.879}
\definecolor{darckGreen}{rgb}{0.1797,0.543,0.3398}
%{\color{lightBlue}Text Here}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}[theorem]{Definition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{example}[theorem]{Example}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remark}
\renewcommand{\abstractname}{\Large\textbf{Abstract}}
\floatname{algorithm}{Algorithm}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\begin{document}
\maketitle
\setcounter{page}{1}
\pagenumbering{arabic}
\section[Section title sans citation]{Conformal Prediction General\cite{angelopoulos2023conformal}}
    \begin{definition}[Exchangeability]\cite{shafer2008tutorial}
        For any r.v. $x_1,\cdots,x_k$, we say they are exchangeable if for any permutation $\sigma:[k]\rightarrow[k]$(bijection), $(x_1,\cdots,x_k)\overset{d.}{=}(x_{\sigma(1)},\cdots,x_{\sigma(k)})$.
    \end{definition}


    \begin{definition}[Weighted Exchangeability]\cite{tibshirani2019conformal}
        For any r.v. $x_1,\cdots,x_k$, we say they are weighted exchangeable if their joint density canbe factorized as
        \begin{equation*}
            f(x_1,\cdots,x_k)=\overset{k}{\underset{i=1}{\prod}}w_i(x_i)\cdot g(x_1,\cdots,x_k),
        \end{equation*}
        where $g$ is exchangeable, i.e., $g(x_1,\cdots,x_k)=g(x_{\sigma(1)},\cdots,x_{\sigma(k)})$.
    \end{definition}


    For conformal prediction two classes of targets are studied.
    \begin{definition}[Marginal Coverage]
        $(X,Y)\in\mathbb{R}^p\times\mathbb{R}\sim P_{XY}$ which is unknown. Given training set $Tr=\{(X_i,Y_i)\}_{i=1}^n$, and test on $(X_{n+1},Y_{n+1})$, both i.i.d.


        $C_\alpha$  satisfies distribution-free marginal coverage at level $1-\alpha$ if
        \begin{equation*}
            P(Y_{n+1}\in C_\alpha(X_{n+1}))\geq 1-\alpha,\ \forall P_{XY}
        \end{equation*}
        The probability is with respect to $\{(X_i,Y_i)\}_{i=1}^{n+1}$.
    \end{definition}


    \begin{definition}[Conditional Coverage]
        $(X,Y)\in\mathbb{R}^p\times\mathbb{R}\sim P_{XY}$ which is unknown. Given training set $Tr=\{(X_i,Y_i)\}_{i=1}^n$, and test on $(X_{n+1},Y_{n+1})$, both i.i.d.


        $C_\alpha$  satisfies distribution-free marginal coverage at level $1-\alpha$ if
        \begin{equation*}
            P\left( Y_{n+1}\in C_\alpha(X_{n+1})\Big|X_{n+1}=x \right)\geq 1-\alpha,\ \forall P_{XY}
        \end{equation*}
        The probability is with respect to $\{(X_i,Y_i)\}_{i=1}^n$ and $Y_{n+1}$.
    \end{definition}


    \begin{definition}[Conformal Score Function]
        For data pair $(X,Y)$ and point predictor and any loss function $V(\cdot,\cdot)$, call $R=S(X,Y)=V(Y,\hat{f}(X))$ be the conformal score(or residual).
    \end{definition}


    \begin{definition}[Efficiency]
        $X$ is some r.v. following the testing distribution and $C_\alpha$ is efficient if $\mathbb{E}\left[ |C_\alpha(X)| \right]$ is small. Define $Size(C_\alpha)=\dfrac{1}{n}\overset{n}{\underset{i=1}\sum}|C_\alpha(X_i)|$.
    \end{definition}


\section{Localized CP Article1}
    Conformalized Quantile Regression
    \cite{romano2019conformalized}


    The core idea is if conditional distribution function $F(y|X=x)$ is known and conditional quantile is $q_\alpha(x)=\inf\{y:F(y|X=x)\geq\alpha\}$, for $\alpha_1=\alpha/2,\ \alpha_2=1-\alpha/2$ we can define conformal set to be $C_\alpha(x)=\left[ q_{\alpha_1}(x),q_{\alpha_2}(x) \right]$. Next is to estimate quantiles from data.


    Follow the split CP setting, 
    \begin{itemize}
        \item First divide training set $D$ into two sets: $D_1$ for proper training set and $D_2$ for calibration set. And let $n_i=|D_i|$, fit point predictor $\hat{q}_{\alpha_1},\ \hat{q}_{\alpha_2}$ on $D_1$.
        \item Calculate cnformity scores on calibration set: $R_i=\max\{\hat{q}_{\alpha_1}(X_i)-Y_i,Y_i-\hat{q}_{\alpha_2}(X_i)\}$ for $i\in D_2$, and $R=\max\{\hat{q}_{\alpha_1}(X_i)-Y_i,Y_i-\hat{q}_{\alpha_2}(X_i)\}$
        \item Find the $\lceil(1-\alpha)(n_2+1)\rceil$-th empirical quantile of $R_i,\ i\in D_2$ as $\hat{q}$ and construct conformal set $C_\alpha(x)=\left[ \hat{q}_{\alpha_1}(x)-\hat{q},\hat{q}_{\alpha_2}(x)+\hat{q} \right]$
    \end{itemize}
    Note that $\left\{ Y\in C_\alpha(X) \right\}=\left\{ R\leq\hat{q} \right\}$. With exchangeability of $R_i,\ i\in D_2$ and $R$ the coverage is assured.


    \begin{remark}
        Can also define $R_1=\hat{q}_{\alpha_1}(X_i)-Y_i,\ R_2=Y_i-\hat{q}_{\alpha_2}(X_i)$ and their $\lceil(1-\alpha)(n_2+1)\rceil$-th empirical quantile $\hat{q}_1,\ \hat{q}_2$. Define conformal set $C_\alpha(X)=\left[ \hat{q}_{\alpha_1}(X)-\hat{q}_1,\hat{q}_{\alpha_2}(X)+\hat{q}_2 \right]$.
    \end{remark}


\section{Localized CP Article2}
    Distribution-Free Predictive Inference For Regression\cite{lei2018distribution}


    The setting is similar here. Consider the split setting, divide training set $D$ into two sets: $D_1$ for proper training set and $D_2$ for calibration set.
    \begin{itemize}
        \item Train $\hat{f}(x)$ on $D_1$ as a point predictor and based on $(X_i,|Y_i-\hat{f}(X_i)|),\ i\in D_1$, train $\hat{\rho}$ as an estimator of conditional MAD $|Y-f(X)|\Big|X=x$. For a given test point $X$ fix trial data $y$.
        \item Calculate scores on calibration set $R_i=\dfrac{|Y_i-\hat{f}(X_i)|}{\hat{\rho}(X_i)},\ i\in D_2,\ R=\dfrac{|y-\hat{f}(X)|}{\hat{\rho}(X)}$ and find the $\lceil(1-\alpha)(n_2+1)\rceil$-th empirical quantile $\hat{q}_\alpha$.
        \item Define conformal set $C_\alpha(X)=\{y:R\leq\hat{q}_\alpha\}$.
    \end{itemize}
    Note that $\left\{ Y\in C_\alpha \right\}=\left\{ R\leq\hat{q}_\alpha \right\}$ and $R,\ R_i,\ i\in D_2$ are exchangeable, it's easy to prove the coverage.


\section{Localized CP Article3}
    Split Localized Conformal Prediction\cite{han2022split}


    If score is defined by $R=|Y-\hat{f}(X)|$, the split CP follows setting $Y=\hat{f}(X)+\varepsilon$ where $\varepsilon$ is independent of $X$. However this is not always true, we need to estimate the distribution of $R|X=x$.


    Follow split CP setting, divide training set $D$ into two sets: $D_1$ for proper training set and $D_2$ for calibration set. 
    
    
    We can estimate the distribution of $R|X=x$ with kernel smoothing. Assume distribution $F(R=r|X=x)=\mathbb{E}\mathbbm{1}(R\leq r|X=x)$, and NW estimator is
    \begin{equation*}
        \hat{F}_h(R=r|X=x)=\overset{}{\underset{i\in D_1}\sum}w_h(X_i|x)\mathbbm{1}\{R_i\leq r\},
    \end{equation*}
    where $w_h(X_i|x)=\dfrac{K(||g(X_i)-g(x)||/h)}{\overset{}{\underset{j\in D_1}\sum}K(||g(X_j)-g(x)||/h)}$ with some embedding function $g$. But directly find the $\alpha$ quantile of $\hat{F}_h(R=r|X=x)$ as $\hat{q}_\alpha$ and construct conformal set as $\left\{ y:R\leq \hat{q}_\alpha \right\}$ cannot guarantee coverage. Further, calculate a residual score on calibration set $R_i'=R_i-Q(\alpha,\hat{F}_h(R|X=X_i)),\ i\in D_2,\ R'=R-Q(\alpha,\hat{F}_h(R|X=X))$. The exchangeability still holds on $R_i'$. The conformal set is $C_\alpha=\left\{ y:R\leq\hat{q}_\alpha' \right\}$, where $\hat{q}_\alpha'$ is $\lceil(1-\alpha)(n_2+1)\rceil$-th empirical quantile of $R_i',\ i\in D_2$. Entire procedure is
    \begin{itemize}
        \item Train point predictor $\hat{f}$ on $D_1$ and calculate score $R_i=|Y_i-\hat{f}(X_i)|,\ i\in D_1$. Choose $h$ to get NW estimator.
        \item Calculate on calibration set $R_i,\ Q(\alpha,\hat{F}_h(R|X=X_i)),\ i\in D_2$ and given $X$, for any trial data $y$, calculate $R$ and $Q(\alpha,\hat{F}_h(R|X))$
        \item Calculate residual score on calibration set $R_i'=R_i-Q(\alpha,\hat{F}_h(R|X=X_i)),\ i\in D_2$ and $R'=R-Q(\alpha,\hat{F}_h(R|X))$. Find $\lceil(1-\alpha)(n_2+1)\rceil$-th empirical quantile of $R_i',\ i\in D_2$
        \item Define conformal set $C_\alpha(X)=\left\{ y:R'\leq\hat{q}_\alpha' \right\}$
    \end{itemize}


    The coverage guarantee comes from $\{Y\in C_\alpha(X)\}=\{R'\leq\hat{q}_\alpha'\}$ and the exchangeability within $R'$ and $R_i',\ i\in D_2$.


\section{Localized CP Article4}
    Localized conformal prediction: a generalized inference framework for conformal prediction\cite{guan2023localized}
    Assume have $Z_1=(X_1,Y_1),\cdots,Z_n=(X_n,Y_n)$ at hand and $Y|X$ varies across different $X$. A new observation $X_{n+1}$ arrives and the conformal set of $Y_{n+1}$ is required. The core idea is that for a given score function $S=V(X,Y)$ and require estimate a correct $\alpha$ quantile of $S|X$.
    
    
    First a natural idea is construct $S|X$'s distribution. Assume any kernel function $K(x_1,x_2)$ and let $p_{i,j}=\dfrac{K(X_i,X_j)}{\overset{n}{\underset{l=1}\sum}K(X_i,X_l)}$. Conditional on $X_i$ assign larger weight to $S_j$ that has $X_j$ near $X_i$. Let $F_i=\overset{n}{\underset{j=1}\sum}p_{i,j}\delta_{S_j}$ which is similar to the first idea of article3. Directly take the $1-\alpha$ quantile cannot guarantee coverage. Take $E_Z=\{\{Z_i\}_{i=1}^{n+1}=\{z_i\}_{i=1}^{n+1}\}$ and with all $Z_i$ drawn i.i.d., $P(Z_{n+1}=z_i|E_Z)=1/(n+1)$. Write $s_{i}$ for each $z_i$. As $E_Z$ means set equivalent, there exists some permutation $\sigma:[n+1]\rightarrow[n+1]$ s.t. $S_i=s_{\sigma(i)}$ condition on $E_Z$. To be specific, $F_i$ is the distribution construct by $S_i,\ i=1,\cdots,n+1$ and $F_i'$ by $s_i,\ i=1,\cdots,n+1$.
    \begin{align*}
        P(S_{n+1}\leq Q(\alpha';F_{n+1})|E_Z)=\overset{n+1}{\underset{i=1}\sum}P(S_{n+1}=s_i|E_Z)\mathbbm{1}\left\{ S_{n+1}\leq Q(\alpha';F_{n+1})|E_Z,S_{n+1}=s_i \right\}.
    \end{align*}
    The most important thing here is $Q(\alpha';F_{n+1})|(E_Z,S_{n+1}=s_i)=Q(\alpha';F_i')|E_Z$. The order of $S_1,\cdots,S_n$ doesn't influence $F_{n+1}$. Thus,
    \begin{align*}
        P(S_{n+1}\leq Q(\alpha';F_{n+1})|E_Z)=\overset{n+1}{\underset{i=1}\sum}P(S_{n+1}=s_i|E_Z)\mathbbm{1}\left\{ s_i\leq Q(\alpha';F_i')|E_Z \right\}.
    \end{align*}
    For any trial data $Y_{n+1}=y$, take $s_i$ and $F_i'$ be calculated based on sample data. Thus find $\alpha'$ that makes $\overset{n+1}{\underset{i=1}\sum}P(S_{n+1}=s_i|E_Z)\mathbbm{1}\left\{ s_i\leq Q(\alpha';F_i') \right\}\geq 1-\alpha$ then take expectation on $E_Z$ and we have
    \begin{align*}
        P(S_{n+1}\leq Q(\alpha';F_{n+1}))\geq 1-\alpha.
    \end{align*}
    Thus the entire process is as follows
    \begin{itemize}
        \item Fix trial data $Y_{n+1}=y$ and calculate score $s_1,\cdots,s_{n+1}$ and $F_1',\cdots,F_{n+1}'$ based on $Z_1,\cdots,Z_n,Z_{n+1}'=(X_{n+1},y)$.
        \item Find $\alpha'$ that makes $\overset{n+1}{\underset{i=1}\sum}P(S_{n+1}=s_i|E_Z)\mathbbm{1}\left\{ s_i\leq Q(\alpha';F_i') \right\}\geq 1-\alpha$.
        \item Include $y$ in conformal set if $s_{n+1}\leq Q(\alpha';F_{n+1}')$. In conclusion $C_\alpha(X_{n+1})=\{y:s_{n+1}\leq Q(\alpha';F_{n+1}')\}$.
    \end{itemize}
    \begin{remark}
        *****!!!!!


        For agent $1,\cdots,K$ each with distribution $P^k$ we can try to find some kernel $K$ to estimate the distance between agent $i,\ j$ and further construct the distribution or find $\alpha'$.
    \end{remark}


\section{Localized CP Article5}
    Conformal prediction with local weights: randomization enables robust guarantees\cite{hore2023conformal}


    Assume using training dataset $(X_1,Y_1),\cdots,(X_n,Y_n)$ and a test point $(X_{n+1},Y_{n+1})$ all come from distribution $P=P_X\times P_{Y|X}$. Given a new $X_{n+1}$, sample $\tilde{X}$ based on $X_{n+1}$ with density $H(X_{n+1},\cdot)$ where $H$ be some kernel function. Thus $X_{n+1},\tilde{X}$ has joint density
    \begin{equation*}
        P_X(X_{n+1})H(X_{n+1},\tilde{X}).
    \end{equation*}
    Conditional on $\tilde{X}$(means be considered as a given constant) and density of $X_{n+1}$ is propotional to $P_X(X_{n+1})H(X_{n+1},\tilde{X})$ and finally the density ratio between $X_{n+1}$ and $X_{i},i\leq n$ is propotional to $H(X_{n+1},\tilde{X})$.


    This means conditional on $\tilde{X}$ and $X_{n+1}$ has a covariate shift $H(X_{n+1},\tilde{X})$ according to the training dataset. Construct empirical distribution
    \begin{equation*}
        \tilde{F}=\overset{n}{\underset{i=1}\sum}\tilde{w}_i\delta_{S_i}+\tilde{w}_{n+1}\delta_{\infty},\ \tilde{w}_i=\dfrac{H(X_i,\tilde{X})}{\overset{n+1}{\underset{j=1}\sum}H(X_j,\tilde{X})}.
    \end{equation*}
    The conformal set is $C_\alpha(X_{n+1})=\{y:S_{n+1}\leq Q(1-\alpha,\tilde{F})\}$.


    The coverage $\mathbb{P}(Y_{n+1}\in C_\alpha(X_{n+1}))\geq 1-\alpha$ according to Tibshirani\cite{tibshirani2019conformal}.


    This method has certain local conditional coverage property. Start with a covariate shift setting.
    \begin{theorem}
        Assume $X_1,\cdots,X_n\sim P_X,X_{n+1}\sim P_X\circ g,Y|X\sim P_{Y|X}$, where $P_X\circ g$ has density $\varpropto P_X(x)g(x)$. $g(x)$ is propotional to the density ratio. Assume $M=||g||_\infty$ and $L_{g,2\epsilon,A}=\underset{x,x'\in A,||x-x'||\leq 2\epsilon}{\sup}|g(x)-g(x')|/2\epsilon$,
        \begin{align*}
            \mathbb{P}\left( Y_{n+1}\in C_\alpha(X_{n+1}) \right)\geq 1-\alpha-\dfrac{\underset{A\subset\mathcal{X},\epsilon>0}{\inf}\left( \epsilon L_{g,2\epsilon,A}+M\left( \mathbb{P}\left( ||X-\tilde{X}||>\epsilon \right)+P_X(A^C) \right) \right)}{E_{P_X}g(X)}.
        \end{align*}
    \end{theorem}
    To see this, fixed $\tilde{X}$ which is generated with kernel $H(\cdot,X_{n+1})$ based on $X_{n+1}\sim P_X\circ g$. Assume $X_{n+1}'|\tilde{X}\sim P_X\circ H(\cdot,\tilde{X}),Y_{n+1}'|X_{n+1}'\sim P_{Y|X}$. Use the marginal coverage property
    \begin{equation}
        \mathbb{P}\left( Y_{n+1}'\in C_\alpha(X_{n+1}')\Big|\tilde{X} \right)\geq 1-\alpha.\label{formula1}
    \end{equation}


    A commonly used lemma is
    \begin{lemma}
        Assume two random variables $Z,Z'$, sharing common sample space $\mathcal{Z}$, have density $p(z)$ and $p'(z')$, the total variation is defined as $d_{TV}(Z,Z')=\int |p(z)-p'(z)|dz/2$. For any $A\subset \mathcal{Z}$,
        \begin{equation*}
            \Big|\mathbb{P}(Z\in A)-\mathbb{P}(Z'\in A)\Big|\leq d_{TV}(Z,Z').
        \end{equation*}
    \end{lemma}


    Consider $\mathbb{P}\left( Y_{n+1}\in C_\alpha(X_{n+1}) \right)$ according to formula \ref{formula1},
    \begin{equation*}
        \mathbb{P}\left( Y_{n+1}\in C_\alpha(X_{n+1})\Big|\tilde{X} \right)\geq 1-\alpha-d_{TV}(X_{n+1},X_{n+1}'),
    \end{equation*}
    note $d_{TV}(X_{n+1},X_{n+1}')$ is calculated conditional on $\tilde{X}$. Condition on $\tilde{X}$, $X_{n+1}$ has density $P_X(x)\circ g(x)\circ H(x,\tilde{X})$, and $X_{n+1}'$ has $P_X(x)\circ H(x,\tilde{X})$, to be specific
    \begin{equation*}
        \dfrac{d(P_X\circ g\circ H(\cdot,\tilde{X}))(x)}{d(P_X\circ H(\cdot,\tilde{X}))(x)}=g(x)\dfrac{\int H(x',\tilde{X})dP_X(x')}{\int g(x')H(x',\tilde{X})dP_X(x')}=\dfrac{g(x)}{E_{X'\sim P_X\circ H(\cdot,\tilde{X})}\left[ g(X') \right]},
    \end{equation*}
    thus as $d_{TV}(p,q)=\int|p(x)-q(x)|dx/2=\int|p(x)/q(x)-1|dQ(x)/2=E_Q|p/q-1|/2$, and the denominator of the last formula only depends on $\tilde{X}$ which is constant condition on $\tilde{X}$, we have
    \begin{align*}
        d_{TV}(X_{n+1},X_{n+1}')&=\dfrac{1}{2}E_{X\sim P_X\circ H(\cdot,\tilde{X})}\left\{ \Big|\dfrac{g(X)}{E_{X'\sim P_X\circ H(\cdot,\tilde{X})}\left[ g(X') \right]}-1\Big| \right\}\\
        &=\dfrac{E_{X\sim P_X\circ H(\cdot,\tilde{X})}\left\{ g(X)-E_{X'\sim P_X\circ H(\cdot,\tilde{X})}\left[ g(X') \right] \right\}}{2E_{X'\sim P_X\circ H(\cdot,\tilde{X})}\left[ g(X') \right]}\\
        \text{(Jensen's Inequality)}&\leq\dfrac{E_{X,X'\sim P_X\circ H(\cdot,\tilde{X})}\Big|g(X)-g(X')\Big|}{2E_{X'\sim P_X\circ H(\cdot,\tilde{X})}\left[ g(X') \right]}\\
        &=\dfrac{E_{P_X}H(X,\tilde{X})}{2E_{P_X}g(X)H(X,\tilde{X})}E_{X,X'\sim P_X\circ H(\cdot,\tilde{X})}\Big|g(X)-g(X')\Big|
    \end{align*}
    Now take expectation on $\tilde{X}$ which has density $d\tilde{P}_{\tilde{X}}(x)=\dfrac{\int H(X,x)P_X(X)g(X)dX}{\int P_X(X)g(X)dX}dx$. Further assume $dP_{\tilde{X}}(x)=\int H(X,x)P_X(X)dXdx$, thus
    \begin{align*}
        &E_{\tilde{X}\sim\tilde{P}_{\tilde{X}}}d_{TV}(X_{n+1},X_{n+1}')\\
        \leq&\int \dfrac{E_{P_X}H(X,\tilde{X})}{2E_{P_X}g(X)H(X,\tilde{X})}\dfrac{\int H(X,\tilde{X})P_X(X)g(X)dX}{\int P_X(X)g(X)dX}E_{X,X'\sim P_X\circ H(\cdot,\tilde{X})}\Big|g(X)-g(X')\Big|d\tilde{X}\\
        =&\int\dfrac{E_{P_X}H(X,\tilde{X})}{2E_{P_X}g(X)}E_{X,X'\sim P_X\circ H(\cdot,\tilde{X})}\Big|g(X)-g(X')\Big|d\tilde{X}\\
        =&\dfrac{E_{\tilde{X}\sim P_{\tilde{X}}}E_{X,X'\sim P_X\circ H(\cdot,\tilde{X})}\Big|g(X)-g(X')\Big|}{2E_{P_X}g(X)},
    \end{align*}
    if $X,X'\in A,|X-X'|<2\epsilon$, $|g(X)-g(X')|\leq 2\epsilon L_{g,2\epsilon,A}$, otherwise $|g(X)-g(X')|\leq 2M$. Write $E_A=\left\{ X,X'\in A,|X-X'|<2\epsilon \right\}$,
    \begin{equation*}
        E_A^C\subset \left\{ X\notin A\text{ or }X'\notin A\text{ or }|g(X)-g(\tilde{X})|>\epsilon\text{ or }|g(X')-g(\tilde{X})|>\epsilon \right\}.
    \end{equation*}
    Thus the probability over $E_A^C$ is bounded, based on $X|\tilde{X}\sim P_{\tilde{X}}$ has distribution $P_X\circ H(\cdot,\tilde{X})$,
    \begin{align*}
        E_{P_{\tilde{X}}}\mathbb{P}(E_A^C)&\leq 2E_{P_{\tilde{X}}}\left\{ \mathbb{P}_{X\sim P_X\circ H(\cdot,\tilde{X})}\left[ X\notin A\text{ or }|g(X)-g(\tilde{X})|>\epsilon \right] \right\}\\
        &\leq 2E_{P_{\tilde{X}}}\left\{ \mathbb{P}_{X\sim P_X\circ H(\cdot,\tilde{X})}\left[ X\notin A\right]+\mathbb{P}_{X\sim P_X\circ H(\cdot,\tilde{X})}\left[ |g(X)-g(\tilde{X})|>\epsilon \right] \right\}\\
        &=2\mathbb{P}_{X\sim P_X}(A^C)+2\mathbb{P}_{X\sim P_X,\tilde{X}\sim P_{\tilde{X}}}\left( |g(X)-g(\tilde{X})|>\epsilon \right),
    \end{align*}
    holds for any $A$, take inferior limit gives the result. 


    Next consider conditional coverage property.
    \begin{theorem}
        Assume $(X_1,Y_1),\cdots,(X_{n+1},Y_{n+1})\overset{i.i.d}{\sim}P$, for any $B\subset\mathcal{X}$, it holds that
        \begin{equation*}
            \mathbb{P}\left( Y_{n+1}\in C_\alpha(X_{n+1})\Big|X_{n+1}\in B \right)\geq 1-\alpha-\dfrac{\underset{\epsilon>0}{\inf}\left\{ P_X(bd_{2\epsilon}(B))+P_{X,\tilde{X}}\left( ||X-\tilde{X}||>\epsilon \right) \right\}}{P_X(B)},
        \end{equation*}
        where $bd_{2\epsilon}(B)=\left\{ x\in B:\underset{x'\in B^C}{\inf}||x-x'||\leq 2\epsilon\right\}$.
    \end{theorem}
    This theorem is easy to prove using previous theorem. Take $g(x)=\mathbbm{1}(x\in B)$ and $A=(bd_{2\epsilon}(B))^C$. For any $x,x'\in A$, either $x,x'\in B$ or $x,x'\in B^C$ which leads to $g(x)=g(x')$ and $L_{g,2\epsilon,A}=0$. Also $E_{P_X}g(X)=P_X(B)$ and $M=||g||_\infty=1$.
\newpage
\bibliographystyle{plain}
\bibliography{ref}
\end{document}